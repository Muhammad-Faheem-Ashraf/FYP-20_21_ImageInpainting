{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"pip install torchvision -y -c pytorch","execution_count":1,"outputs":[{"output_type":"stream","text":"\nUsage:   \n  /opt/conda/bin/python3.7 -m pip install [options] <requirement specifier> [package-index-options] ...\n  /opt/conda/bin/python3.7 -m pip install [options] -r <requirements file> [package-index-options] ...\n  /opt/conda/bin/python3.7 -m pip install [options] [-e] <vcs project url> ...\n  /opt/conda/bin/python3.7 -m pip install [options] [-e] <local project path> ...\n  /opt/conda/bin/python3.7 -m pip install [options] <archive url/path> ...\n\nno such option: -y\nNote: you may need to restart the kernel to use updated packages.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev","execution_count":2,"outputs":[{"output_type":"stream","text":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n100  5116  100  5116    0     0   9387      0 --:--:-- --:--:-- --:--:--  9369\nUpdating... This may take around 2 minutes.\nUpdating TPU runtime to pytorch-dev20200515 ...\nFound existing installation: torch 1.5.0\nUninstalling torch-1.5.0:\n  Successfully uninstalled torch-1.5.0\nFound existing installation: torchvision 0.6.0a0+35d732a\nUninstalling torchvision-0.6.0a0+35d732a:\nDone updating TPU runtime\n  Successfully uninstalled torchvision-0.6.0a0+35d732a\nCopying gs://tpu-pytorch/wheels/torch-nightly+20200515-cp37-cp37m-linux_x86_64.whl...\n\\ [1 files][ 91.0 MiB/ 91.0 MiB]                                                \nOperation completed over 1 objects/91.0 MiB.                                     \nCopying gs://tpu-pytorch/wheels/torch_xla-nightly+20200515-cp37-cp37m-linux_x86_64.whl...\n| [1 files][119.5 MiB/119.5 MiB]                                                \nOperation completed over 1 objects/119.5 MiB.                                    \nCopying gs://tpu-pytorch/wheels/torchvision-nightly+20200515-cp37-cp37m-linux_x86_64.whl...\n/ [1 files][  2.3 MiB/  2.3 MiB]                                                \nOperation completed over 1 objects/2.3 MiB.                                      \nProcessing ./torch-nightly+20200515-cp37-cp37m-linux_x86_64.whl\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch==nightly+20200515) (0.18.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch==nightly+20200515) (1.18.5)\n\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n\u001b[31mERROR: kornia 0.3.1 has requirement torch==1.5.0, but you'll have torch 1.6.0a0+bf2bbd9 which is incompatible.\u001b[0m\n\u001b[31mERROR: allennlp 1.0.0 has requirement torch<1.6.0,>=1.5.0, but you'll have torch 1.6.0a0+bf2bbd9 which is incompatible.\u001b[0m\nInstalling collected packages: torch\nSuccessfully installed torch-1.6.0a0+bf2bbd9\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.3.3 is available.\nYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\nProcessing ./torch_xla-nightly+20200515-cp37-cp37m-linux_x86_64.whl\nInstalling collected packages: torch-xla\nSuccessfully installed torch-xla-1.6+2b2085a\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.3.3 is available.\nYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\nProcessing ./torchvision-nightly+20200515-cp37-cp37m-linux_x86_64.whl\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision==nightly+20200515) (1.18.5)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torchvision==nightly+20200515) (1.6.0a0+bf2bbd9)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision==nightly+20200515) (7.2.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->torchvision==nightly+20200515) (0.18.2)\nInstalling collected packages: torchvision\nSuccessfully installed torchvision-0.7.0a0+a6073f0\n\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.3.3 is available.\nYou should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following additional packages will be installed:\n  libgfortran4 libopenblas-base\nThe following NEW packages will be installed:\n  libgfortran4 libomp5 libopenblas-base libopenblas-dev\n0 upgraded, 4 newly installed, 0 to remove and 59 not upgraded.\nNeed to get 8550 kB of archives.\nAfter this operation, 97.6 MB of additional disk space will be used.\nGet:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libgfortran4 amd64 7.5.0-3ubuntu1~18.04 [492 kB]\nGet:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopenblas-base amd64 0.2.20+ds-4 [3964 kB]\nGet:3 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopenblas-dev amd64 0.2.20+ds-4 [3860 kB]\nGet:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libomp5 amd64 5.0.1-1 [234 kB]\nFetched 8550 kB in 0s (19.6 MB/s)\ndebconf: delaying package configuration, since apt-utils is not installed\nSelecting previously unselected package libgfortran4:amd64.\n(Reading database ... 107745 files and directories currently installed.)\nPreparing to unpack .../libgfortran4_7.5.0-3ubuntu1~18.04_amd64.deb ...\nUnpacking libgfortran4:amd64 (7.5.0-3ubuntu1~18.04) ...\nSelecting previously unselected package libopenblas-base:amd64.\nPreparing to unpack .../libopenblas-base_0.2.20+ds-4_amd64.deb ...\nUnpacking libopenblas-base:amd64 (0.2.20+ds-4) ...\nSelecting previously unselected package libopenblas-dev:amd64.\nPreparing to unpack .../libopenblas-dev_0.2.20+ds-4_amd64.deb ...\nUnpacking libopenblas-dev:amd64 (0.2.20+ds-4) ...\nSelecting previously unselected package libomp5:amd64.\nPreparing to unpack .../libomp5_5.0.1-1_amd64.deb ...\nUnpacking libomp5:amd64 (5.0.1-1) ...\nSetting up libomp5:amd64 (5.0.1-1) ...\nSetting up libgfortran4:amd64 (7.5.0-3ubuntu1~18.04) ...\nSetting up libopenblas-base:amd64 (0.2.20+ds-4) ...\nupdate-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/libblas.so.3 to provide /usr/lib/x86_64-linux-gnu/libblas.so.3 (libblas.so.3-x86_64-linux-gnu) in auto mode\nupdate-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/liblapack.so.3 to provide /usr/lib/x86_64-linux-gnu/liblapack.so.3 (liblapack.so.3-x86_64-linux-gnu) in auto mode\nSetting up libopenblas-dev:amd64 (0.2.20+ds-4) ...\nupdate-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/libblas.so to provide /usr/lib/x86_64-linux-gnu/libblas.so (libblas.so-x86_64-linux-gnu) in auto mode\nupdate-alternatives: using /usr/lib/x86_64-linux-gnu/openblas/liblapack.so to provide /usr/lib/x86_64-linux-gnu/liblapack.so (liblapack.so-x86_64-linux-gnu) in auto mode\nProcessing triggers for libc-bin (2.27-3ubuntu1) ...\n","name":"stdout"}]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"USE_TPU = True\nMULTI_CORE = False\n\nimport os\nimport torch\n\n \nDATA_DIR = '../input/imagenet-mni/imagenet/'\nOUT_DIR = './result/'\nMODEL_DIR = './models/'\nCHECKPOINT_DIR = './'\n\nTRAIN_DIR = DATA_DIR+\"train/\"  # UPDATE\nTEST_DIR = DATA_DIR+\"test/\" # UPDATE\n\nos.makedirs(TRAIN_DIR, exist_ok=True)\nos.makedirs(TEST_DIR, exist_ok=True)\nos.makedirs(MODEL_DIR, exist_ok=True)\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)\nos.makedirs(OUT_DIR, exist_ok=True)\n\n# DATA INFORMATION\nIMAGE_SIZE = 224\nBATCH_SIZE = 10\nGRADIENT_PENALTY_WEIGHT = 10\nNUM_EPOCHS = 1\nKEEP_CKPT = 2\n# save_model_path = MODEL_DIR\nDEVICE=0\nif USE_TPU:\n    import torch_xla.core.xla_model as xm\n    if not MULTI_CORE:\n        DEVICE = xm.xla_device()\n\nif not USE_TPU:\n    if torch.cuda.is_available():\n        DEVICE = torch.device('cuda')\n    else:\n        DEVICE = 'cpu'\n\n\n","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport cv2 \nimport numpy as np\n\n\nclass DATA():\n    def __init__(self, dirname, max_len=None):\n        self.dir_path = dirname\n        self.filelist = os.listdir(self.dir_path)[:max_len]\n        self.batch_size =  BATCH_SIZE\n        self.size = len(self.filelist)\n        self.data_index = 0\n    def __len__(self):\n        return len(self.filelist)\n    \n    def __getitem__(self, item):\n        img = []\n        label = []\n        itemfilelist = ''\n        filename = os.path.join(self.dir_path, self.filelist[item])\n        itemfilelist = self.filelist[item]\n        greyimg, colorimg = self.read_img(filename)\n        img = greyimg\n        label = colorimg\n        img = np.asarray(img)/255 # values between 0 and 1\n        label = np.asarray(label)/255 # values between 0 and 1\n        return img, label, itemfilelist\n\n    def read_img(self, filename):\n        img = cv2.imread(filename, 3)\n        height, width, channels = img.shape\n        min_hw = int(min(height,width)/2)\n        img = img[int(height/2)-min_hw:int(height/2)+min_hw,int(width/2)-min_hw:int(width/2)+min_hw,:]\n        labimg = cv2.cvtColor(cv2.resize(img, ( IMAGE_SIZE,  IMAGE_SIZE)), cv2.COLOR_RGB2Lab) ## Changed BGR to RGB\n        return np.reshape(labimg[:,:,0], (1,  IMAGE_SIZE,  IMAGE_SIZE)), np.reshape(labimg[:, :, 1:], (2, IMAGE_SIZE,  IMAGE_SIZE))\n\n    def generate_batch(self):\n        batch = []\n        labels = []\n        filelist = []\n        for i in range(self.batch_size):\n            filename = os.path.join(self.dir_path, self.filelist[self.data_index])\n            filelist.append(self.filelist[self.data_index])\n            greyimg, colorimg = self.read_img(filename)\n            batch.append(greyimg)\n            labels.append(colorimg)\n            self.data_index = (self.data_index + 1) % self.size\n        batch = np.asarray(batch)/255 # values between 0 and 1\n        labels = np.asarray(labels)/255 # values between 0 and 1\n        return batch, labels, filelist\n","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\nimport os\nimport cv2\nimport torch \nimport glob\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nif  USE_TPU:\n    import torch_xla.core.xla_model as xm\n\ndef preprocess(imgs):\n    try:\n        imgs = imgs.detach().numpy()\n    except:\n        pass\n    imgs = imgs * 255\n    imgs[imgs>255] = 255\n    imgs[imgs<0] = 0 \n    return imgs.astype(np.uint8) # torch.unit8\n\ndef reconstruct(batchX, predictedY, filelist):\n\n    batchX = batchX.reshape(224,224,1) \n    predictedY = predictedY.reshape(224,224,2)\n    result = np.concatenate((batchX, predictedY), axis=2)\n    result = cv2.cvtColor(result, cv2.COLOR_Lab2RGB)\n    save_results_path =  OUT_DIR\n    if not os.path.exists(save_results_path):\n        os.makedirs(save_results_path)\n    save_path = os.path.join(save_results_path, filelist +  \"_reconstructed.jpg\" )\n    cv2.imwrite(save_path, result)\n    return result\n    \ndef reconstruct_no(batchX, predictedY):\n\n    batchX = batchX.reshape(224,224,1) \n    predictedY = predictedY.reshape(224,224,2)\n    \n    result = np.concatenate((batchX, predictedY), axis=2)\n    result = cv2.cvtColor(result, cv2.COLOR_Lab2RGB)\n    return result\n\n\ndef imag_gird(axrow, orig, batchL, preds, epoch):\n    fig , ax = plt.subplots(1,3, figsize=(15,15))\n    ax[0].imshow(orig)\n    ax[0].set_title('Original Image')\n\n    ax[1].imshow(np.tile(batchL,(1,1,3)))\n    ax[1].set_title('L Image with Channels reapeated(Input)') \n\n    ax[2].imshow(preds)\n    ax[2].set_title('Pred Image')\n    plt.savefig(f'sample_preds_{epoch}')\n    plt.close()\n  # plt.show()\n\ndef plot_some(test_data, colorization_model, device, epoch):\n    with torch.no_grad():\n        indexes = [0, 2, 9]\n        for idx in indexes: \n            batchL, realAB, filename = test_data[idx]\n            filepath =  TRAIN_DIR+filename\n            batchL = batchL.reshape(1,1,224,224)\n            realAB = realAB.reshape(1,2,224,224)\n            batchL_3 = torch.tensor(np.tile(batchL, [1, 3, 1, 1]))\n            batchL_3 = batchL_3.to(device)\n            batchL = torch.tensor(batchL).to(device).double()\n            realAB = torch.tensor(realAB).to(device).double()\n\n            colorization_model.eval()\n            batch_predAB, _ = colorization_model(batchL_3)\n            img = cv2.imread(filepath)\n            batch_predAB = batch_predAB.cpu().numpy().reshape((224,224,2))\n            batchL = batchL.cpu().numpy().reshape((224,224,1))\n            realAB = realAB.cpu().numpy().reshape((224,224,2))\n            orig = cv2.imread(filepath)\n            orig = cv2.resize(cv2.cvtColor(orig, cv2.COLOR_BGR2RGB), (224,224))\n            # orig = reconstruct_no(preprocess(batchL), preprocess(realAB))\n            preds = reconstruct_no(preprocess(batchL), preprocess(batch_predAB))\n            imag_gird(0, orig, batchL, preds, epoch)\n            plt.show()\n\ndef create_checkpoint(epoch, netG, optG, netD, optD, max_checkpoint, save_path= CHECKPOINT_DIR):\n    print('Saving Model and Optimizer weights.....')\n    checkpoint = {\n        'epoch' : epoch,\n        'generator_state_dict' :netG.state_dict(),\n        'generator_optimizer': optG.state_dict(),\n        'discriminator_state_dict': netD.state_dict(),\n        'discriminator_optimizer': optD.state_dict()\n    }\n    if  USE_TPU:\n        xm.save(checkpoint, '2_checkpoint.pt')\n    else:\n        torch.save(checkpoint, '2_checkpoint.pt')\n    print('Weights Saved !!')\n    del checkpoint\n    files = glob.glob(os.path.expanduser(f\"{save_path}*\"))\n    sorted_files = sorted(files, key=lambda t: -os.stat(t).st_mtime)\n    if len(sorted_files) > max_checkpoint:\n        os.remove(sorted_files[-1])\n\n\n\ndef load_checkpoint(checkpoint_directory, netG, optG, netD, optD, device):\n    load_from_checkpoint = False\n    files = glob.glob(os.path.expanduser(f\"{checkpoint_directory}*\"))\n    for file in files:\n        if file.endswith('.pt'):\n            load_from_checkpoint=True\n            break\n\n    if load_from_checkpoint:\n        print('Loading Model and optimizer states from checkpoint....')\n        sorted_files = sorted(files, key=lambda t: -os.stat(t).st_mtime)\n        checkpoint = torch.load(f'{sorted_files[0]}')\n        epoch_checkpoint = checkpoint['epoch'] + 1\n        netG.load_state_dict(checkpoint['generator_state_dict'])\n        netG.to(device)\n\n        optG.load_state_dict(checkpoint['generator_optimizer'])\n\n        netD.load_state_dict(checkpoint['discriminator_state_dict'])\n        netD.to(device)\n\n        optD.load_state_dict(checkpoint['discriminator_optimizer'])\n        print('Loaded States !!!')\n        print(f'It looks like the this states belong to epoch {epoch_checkpoint-1}.')\n        print(f'so the model will train for { NUM_EPOCHS - (epoch_checkpoint-1)} more epochs.')\n         \n\n        return netG, optG, netD, optD, epoch_checkpoint\n    else:\n        print('There are no checkpoints in the mentioned directoy, the Model will train from scratch.')\n        epoch_checkpoint = 1\n        return netG, optG, netD, optD, epoch_checkpoint\n    \n\n\ndef plot_gan_loss(G_losses, D_losses):\n    plt.figure(figsize=(10,5))\n    plt.title(f\"Generator and Discriminator Loss During Training \")\n    plt.plot(G_losses,label=\"G\")\n    plt.plot(D_losses,label=\"D\")\n    plt.xlabel(\"iterations\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.savefig(f'GANLOSS{epoch}.pdf',figsize=(30,30))\n","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\n\n\nbias=True\n\nclass discriminator_model(nn.Module):\n\n    def __init__(self):\n        super(discriminator_model, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=(4,4),padding=1,stride=(2,2),bias=bias) # 64, 112, 112\n        self.conv2 = nn.Conv2d(64, 128, kernel_size=(4,4), padding=1, stride=(2,2), bias=bias) # 128, 56, 56\n        self.conv3 = nn.Conv2d(128,256, kernel_size=(4,4), padding=1, stride=(2,2), bias=bias) # 256, 28, 28, 2\n        self.conv4 = nn.Conv2d(256,512, kernel_size=(4,4), padding=3, stride=(1,1), bias=bias) # 512, 28, 28\n        self.conv5 = nn.Conv2d(512,1, kernel_size=(4,4), padding=3, stride=(1,1), bias=bias) # 1, \n        self.leaky_relu = nn.LeakyReLU(0.3)\n\n    def forward(self,input):\n\n        net = self.conv1(input)               #[-1, 64, 112, 112]\n        net = self.leaky_relu(net)          #[-1, 64, 112, 112]    \n        net = self.conv2(net)               #[-1, 128, 56, 56] \n        net = self.leaky_relu(net)          #[-1, 128, 56, 56] \n        net = self.conv3(net)               #[-1, 256, 28, 28]\n        net = self.leaky_relu(net)          #[-1, 256, 28, 28]   \n        net = self.conv4(net)               #[-1, 512, 27, 27]\n        net = self.leaky_relu(net)          #[-1, 512, 27, 27]\n        net = self.conv5(net)               #[-1, 1, 26, 26]\n        return net\n\nclass colorization_model(nn.Module):\n    def __init__(self):\n        super(colorization_model, self).__init__()\n\n        self.VGG_model = torchvision.models.vgg16(pretrained=True)\n        self.VGG_model = nn.Sequential(*list(self.VGG_model.features.children())[:-8]) #[None, 512, 28, 28]\n        self.VGG_model = self.VGG_model.double()\n        self.relu = nn.ReLU()\n        self.lrelu = nn.LeakyReLU(0.3)\n        self.global_features_conv1 = nn.Conv2d(512, 512, kernel_size=(3,3), padding=1, stride=(2,2), bias=bias) #[None, 512, 14, 14]\n        self.global_features_bn1 = nn.BatchNorm2d(512,eps=0.001,momentum=0.99)\n        self.global_features_conv2 = nn.Conv2d(512, 512, kernel_size=(3,3), padding=1, stride=(1,1), bias=bias) #[None, 512, 14, 14]\n        self.global_features_bn2 = nn.BatchNorm2d(512,eps=0.001,momentum=0.99)\n        self.global_features_conv3 = nn.Conv2d(512, 512, kernel_size=(3,3), padding=1, stride=(2,2), bias=bias) #[None, 512, 7, 7]\n        self.global_features_bn3 = nn.BatchNorm2d(512,eps=0.001,momentum=0.99)\n        self.global_features_conv4 = nn.Conv2d(512, 512, kernel_size=(3,3), padding=1, stride=(1,1), bias=bias) #[None, 512, 7, 7]\n        self.global_features_bn4 = nn.BatchNorm2d(512,eps=0.001,momentum=0.99)\n\n        self.global_features2_flatten = nn.Flatten()\n        self.global_features2_dense1 = nn.Linear(512*7*7,1024)\n        self.midlevel_conv1 = nn.Conv2d(512,512, kernel_size=(3,3), padding=1, stride=(1,1), bias=bias) #[None, 512, 28, 28]\n        self.global_features2_dense2 = nn.Linear(1024,512)\n        self.midlevel_bn1 = nn.BatchNorm2d(512, eps=0.001,momentum=0.99)\n        self.global_features2_dense3 = nn.Linear(512,256)\n        self.midlevel_conv2 = nn.Conv2d(512,256, kernel_size=(3,3), padding=1, stride=(1,1), bias=bias)\n\n        self.midlevel_bn2 = nn.BatchNorm2d(256,eps=0.001,momentum=0.99)\n\n         #[None, 256, 28, 28]\n        # self.midlevel_bn2 = nn.BatchNorm2d(256)#,,eps=0.001,momentum=0.99)\n\n        self.global_featuresClass_flatten = nn.Flatten()\n        self.global_featuresClass_dense1 = nn.Linear(512*7*7, 4096)\n        self.global_featuresClass_dense2 = nn.Linear(4096, 4096)\n        self.global_featuresClass_dense3 = nn.Linear(4096, 1000)\n        self.softmax = nn.Softmax()\n\n        self.outputmodel_conv1 = nn.Conv2d(512, 256, kernel_size=(1,1), padding=0, stride=(1,1),  bias=bias) \n        self.outputmodel_conv2 = nn.Conv2d(256, 128, kernel_size=(3,3), padding=1, stride=(1,1), bias=bias)\n        self.outputmodel_conv3 = nn.Conv2d(128, 64, kernel_size=(3,3), padding=1, stride=(1,1), bias=bias)\n        self.outputmodel_conv4 = nn.Conv2d(64, 64, kernel_size=(3,3), padding=1, stride=(1,1), bias=bias)\n        self.outputmodel_conv5 = nn.Conv2d(64, 32, kernel_size=(3,3), padding=1, stride=(1,1), bias=bias)\n        self.outputmodel_conv6 = nn.Conv2d(32, 2, kernel_size=(3,3), padding=1, stride=(1,1), bias=bias)\n        self.outputmodel_upsample = nn.Upsample(scale_factor=(2,2))\n        self.outputmodel_bn1 = nn.BatchNorm2d(128)\n        self.outputmodel_bn2 = nn.BatchNorm2d(64)\n        self.sigmoid = nn.Sigmoid()\n        self.tanh = nn.Tanh()\n\n    def forward(self,input_img):\n\n        # VGG Without Top Layers\n\n        vgg_out = self.VGG_model(torch.tensor(input_img).double())\n\n        #Global Features\n\n        global_features = self.relu(self.global_features_conv1(vgg_out))  #[None, 512, 14, 14]\n        global_features = self.global_features_bn1(global_features) #[None, 512, 14, 14]\n        global_features = self.relu(self.global_features_conv2(global_features)) #[None, 512, 14, 14]\n        global_features = self.global_features_bn2(global_features) #[None, 512, 14, 14]\n\n        global_features = self.relu(self.global_features_conv3(global_features)) #[None, 512, 7, 7]\n        global_features = self.global_features_bn3(global_features)  #[None, 512, 7, 7]\n        global_features = self.relu(self.global_features_conv4(global_features)) #[None, 512, 7, 7]\n        global_features = self.global_features_bn4(global_features) #[None, 512, 7, 7]\n\n        global_features2 = self.global_features2_flatten(global_features) #[None, 512*7*7]\n\n        global_features2 = self.global_features2_dense1(global_features2) #[None, 1024]\n        global_features2 = self.global_features2_dense2(global_features2) #[None, 512]\n        global_features2 = self.global_features2_dense3(global_features2) #[None, 256]\n        global_features2 = global_features2.unsqueeze(2).expand(-1,256,28*28) #[None, 256, 784]\n        global_features2 = global_features2.view((-1,256,28,28)) #[None, 256, 28, 28]\n\n        global_featureClass = self.global_featuresClass_flatten(global_features) #[None, 512*7*7]\n        global_featureClass = self.global_featuresClass_dense1(global_featureClass) #[None, 4096]\n        global_featureClass = self.global_featuresClass_dense2(global_featureClass) #[None, 4096]\n        global_featureClass = self.softmax(self.global_featuresClass_dense3(global_featureClass))#[None, 1000]\n\n        # Mid Level Features\n        midlevel_features = self.midlevel_conv1(vgg_out.double()) #[None, 512, 28, 28]\n        midlevel_features = self.midlevel_bn1(midlevel_features) #[None, 512, 28, 28]\n        midlevel_features = self.midlevel_conv2(midlevel_features) #[None, 256, 28, 28]\n        midlevel_features = self.midlevel_bn2(midlevel_features) #[None, 256, 28, 28]\n\n        # Fusion of (VGG16 + MidLevel) + (VGG16 + Global)\n\n        modelFusion = torch.cat([midlevel_features, global_features2],dim=1)\n\n        # Fusion Colorization\n\n        outputmodel = self.relu(self.outputmodel_conv1(modelFusion)) # None, 256, 28, 28\n        outputmodel = self.relu(self.outputmodel_conv2(outputmodel)) # None, 128, 28, 28\n\n        outputmodel = self.outputmodel_upsample(outputmodel) # None, 128, 56, 56\n        outputmodel = self.outputmodel_bn1(outputmodel) # None, 128, 56, 56\n        outputmodel = self.relu(self.outputmodel_conv3(outputmodel)) # None, 64, 56, 56\n        outputmodel = self.relu(self.outputmodel_conv4(outputmodel)) # None, 64, 56, 56 \n\n        outputmodel = self.outputmodel_upsample(outputmodel) # None, 64, 112, 112\n        outputmodel = self.outputmodel_bn2(outputmodel) # None, 64, 112, 112\n        outputmodel = self.relu(self.outputmodel_conv5(outputmodel)) # None, 32, 112, 112\n        outputmodel = self.sigmoid(self.outputmodel_conv6(outputmodel)) # None, 2, 112, 112\n        outputmodel = self.outputmodel_upsample(outputmodel) # None, 2, 224, 224\n\n        return outputmodel, global_featureClass\n\n\nclass GAN(nn.Module):\n    def __init__(self, netG, netD):\n        super(GAN, self).__init__()\n\n        self.netG = netG\n        self.netD = netD\n\n    def forward(self, trainL, trainL_3):\n\n        for param in self.netD.parameters():\n            param.requires_grad= False\n\n        predAB, classVector = self.netG(trainL_3)\n        predLAB = torch.cat([trainL, predAB], dim=1)\n        discpred = self.netD(predLAB)\n\n        return predAB, classVector, discpred\n\n","execution_count":6,"outputs":[{"output_type":"stream","text":"[('__call__', <function LevelMapper.__call__ at 0x7fcf936e1050>), ('__init__', <function LevelMapper.__init__ at 0x7fcf936dbf80>)]\n[('__call__', <function BalancedPositiveNegativeSampler.__call__ at 0x7fcf935787a0>), ('__init__', <function BalancedPositiveNegativeSampler.__init__ at 0x7fcf93578710>)]\n[('__init__', <function BoxCoder.__init__ at 0x7fcf93585f80>), ('decode', <function BoxCoder.decode at 0x7fcf9358a170>), ('decode_single', <function BoxCoder.decode_single at 0x7fcf9358a200>), ('encode', <function BoxCoder.encode at 0x7fcf9358a050>), ('encode_single', <function BoxCoder.encode_single at 0x7fcf9358a0e0>)]\n[('__call__', <function Matcher.__call__ at 0x7fcf93585ef0>), ('__init__', <function Matcher.__init__ at 0x7fcf93585d40>), ('set_low_quality_matches_', <function Matcher.set_low_quality_matches_ at 0x7fcf93585e60>)]\n[('__init__', <function ImageList.__init__ at 0x7fcf9358a4d0>), ('to', <function ImageList.to at 0x7fcf9358a440>)]\n[('__init__', <function Timebase.__init__ at 0x7fcf933d78c0>)]\n[('__init__', <function VideoMetaData.__init__ at 0x7fcf933de950>)]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":" \nimport torch\nimport numpy as np\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nif  USE_TPU:\n    import torch_xla\n    import torch_xla.core.xla_model as xm\n    import torch_xla.distributed.parallel_loader as pl\n    import torch_xla.distributed.xla_multiprocessing as xmp\n\n\n\ndef train(train_loader, GAN_Model, netD, VGG_MODEL, optG, optD, device, losses):\n    \n    batch = 0\n    \n    def wgan_loss(prediction, real_or_not):\n        if real_or_not:\n            return -torch.mean(prediction.float())\n        else:\n            return torch.mean(prediction.float())\n\n    def gp_loss(y_pred, averaged_samples, gradient_penalty_weight):\n\n        gradients = torch.autograd.grad(y_pred,averaged_samples,\n                                  grad_outputs=torch.ones(y_pred.size(), device=device),\n                                  create_graph=True, retain_graph=True, only_inputs=True)[0]\n        gradients = gradients.view(gradients.size(0), -1)\n        gradient_penalty = (((gradients+1e-16).norm(2, dim=1) - 1) ** 2).mean() * gradient_penalty_weight\n        return gradient_penalty\n    for trainL, trainAB, _ in tqdm(iter(train_loader)):\n        batch += 1  \n\n        trainL_3 = torch.tensor(np.tile(trainL, [1,3,1,1]), device=device)\n\n        trainL = torch.tensor(trainL, device=device).double()\n        trainAB = torch.tensor(trainAB, device=device).double()\n        # trainL_3 = trainL_3.to(device).double()\n\n        predictVGG = F.softmax(VGG_MODEL(trainL_3))\n\n        ############ GAN MODEL ( Training Generator) ###################\n        optG.zero_grad()\n        predAB, classVector, discpred = GAN_Model(trainL, trainL_3)\n        D_G_z1 = discpred.mean().item()\n        Loss_KLD = nn.KLDivLoss(size_average='False')(classVector.log().float(), predictVGG.detach().float()) * 0.003\n        Loss_MSE = nn.MSELoss()(predAB.float(), trainAB.float())\n        Loss_WL = wgan_loss(discpred.float(), True) * 0.1 \n        Loss_G = Loss_KLD + Loss_MSE + Loss_WL\n        Loss_G.backward()\n\n        if  USE_TPU:\n            if  MULTI_CORE:\n                xm.optimizer_step(optG)\n            else:\n                xm.optimizer_step(optG, barrier=True)\n        else:\n            optG.step()\n\n        losses['G_losses'].append(Loss_G.item())\n        losses['EPOCH_G_losses'].append(Loss_G.item())\n\n\n\n        ################################################################\n\n        ############### Discriminator Training #########################\n\n        for param in netD.parameters():\n            param.requires_grad = True\n\n        optD.zero_grad()\n        predLAB = torch.cat([trainL, predAB], dim=1)\n        discpred = netD(predLAB.detach())\n        D_G_z2 = discpred.mean().item()\n        realLAB = torch.cat([trainL, trainAB], dim=1)\n        discreal = netD(realLAB)\n        D_x = discreal.mean().item()\n\n        weights = torch.randn((trainAB.size(0),1,1,1), device=device)          \n        averaged_samples = (weights * trainAB ) + ((1 - weights) * predAB.detach())\n        averaged_samples = torch.autograd.Variable(averaged_samples, requires_grad=True)\n        avg_img = torch.cat([trainL, averaged_samples], dim=1)\n        discavg = netD(avg_img)\n\n        Loss_D_Fake = wgan_loss(discpred, False)\n        Loss_D_Real = wgan_loss(discreal, True)\n        Loss_D_avg = gp_loss(discavg, averaged_samples,  GRADIENT_PENALTY_WEIGHT)\n\n        Loss_D = Loss_D_Fake + Loss_D_Real + Loss_D_avg\n        Loss_D.backward()\n        if  USE_TPU:\n            if  MULTI_CORE:\n                xm.optimzer_step(optD)\n            else:\n                xm.optimizer_step(optD, barrier=True)\n        else:\n            optD.step()\n\n        losses['D_losses'].append(Loss_D.item())\n        losses['EPOCH_D_losses'].append(Loss_D.item())\n        # Output training stats\n        if batch % 100 == 0:\n            print('Loss_D: %.8f | Loss_G: %.8f | D(x): %.8f | D(G(z)): %.8f / %.8f | MSE: %.8f | KLD: %.8f | WGAN_F(G): %.8f | WGAN_F(D): %.8f | WGAN_R(D): %.8f | WGAN_A(D): %.8f'\n                % (Loss_D.item(), Loss_G.item(), D_x, D_G_z1, D_G_z2,Loss_MSE.item(),Loss_KLD.item(),Loss_WL.item(), Loss_D_Fake.item(), Loss_D_Real.item(), Loss_D_avg.item()))\n\n","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":" \nimport time\nimport torch\nimport torchvision\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport gc\n\nif  USE_TPU:\n    import torch_xla\n    import torch_xla.core.xla_model as xm\n    import torch_xla.distributed.parallel_loader as pl\n    import torch_xla.distributed.xla_multiprocessing as xmp\n\n\ndef map_fn(index=None, flags=None):\n    global DEVICE\n    torch.set_default_tensor_type('torch.FloatTensor')\n    torch.manual_seed(1234)\n\n    train_data =  DATA( TRAIN_DIR) \n\n    if  MULTI_CORE:\n        train_sampler = torch.utils.data.distributed.DistributedSampler(\n          train_data,\n          num_replicas=xm.xrt_world_size(),\n          rank=xm.get_ordinal(),\n          shuffle=True)\n    else:\n        train_sampler = torch.utils.data.RandomSampler(train_data)\n\n    train_loader = torch.utils.data.DataLoader(\n      train_data,\n      batch_size=flags['batch_size'] if  MULTI_CORE else  BATCH_SIZE,\n      sampler=train_sampler,\n      num_workers=flags['num_workers'] if  MULTI_CORE else 4,\n      drop_last=True,\n      pin_memory=True)\n\n    if  MULTI_CORE:\n        DEVICE = xm.xla_device()\n    else:\n        DEVICE =  DEVICE\n\n\n    netG =  colorization_model().double()\n    netD =  discriminator_model().double()\n\n    VGG_modelF = torchvision.models.vgg16(pretrained=True).double()\n    VGG_modelF.requires_grad_(False)\n\n    netG = netG.to(DEVICE)\n    netD = netD.to(DEVICE)\n\n    VGG_modelF = VGG_modelF.to(DEVICE)\n\n    optD = torch.optim.Adam(netD.parameters(), lr=2e-4, betas=(0.5, 0.999))\n    optG = torch.optim.Adam(netG.parameters(), lr=2e-4, betas=(0.5, 0.999))\n    \n    ## Trains\n    train_start = time.time()\n    losses = {\n      'G_losses' : [],\n      'D_losses' : [],\n      'EPOCH_G_losses' : [],\n      'EPOCH_D_losses' : [],\n      'G_losses_eval' : []\n    }\n\n    netG, optG, netD, optD, epoch_checkpoint =  load_checkpoint( CHECKPOINT_DIR, netG, optG, netD, optD, DEVICE)\n    netGAN = GAN(netG, netD)\n    for epoch in range(NUM_EPOCHS):#(epoch_checkpoint,flags['num_epochs']+1 if  MULTI_CORE else  NUM_EPOCHS+1):\n        print('\\n')\n        print('#'*8,f'EPOCH-{epoch}','#'*8)\n        losses['EPOCH_G_losses'] = []\n        losses['EPOCH_D_losses'] = []\n        if  MULTI_CORE:\n            para_train_loader = pl.ParallelLoader(train_loader, [DEVICE]).per_device_loader(DEVICE)\n            train(para_train_loader, netGAN, netD, VGG_modelF, optG, optD, device=DEVICE, losses=losses)\n            elapsed_train_time = time.time() - train_start\n            print(\"Process\", index, \"finished training. Train time was:\", elapsed_train_time) \n        else:\n            train(train_loader, netGAN, netD, VGG_modelF, optG, optD, device=DEVICE, losses=losses)\n        #########################CHECKPOINTING#################################\n        create_checkpoint(epoch, netG, optG, netD, optD, max_checkpoint= KEEP_CKPT, save_path =  CHECKPOINT_DIR)\n        ########################################################################\n        plot_some(train_data, netG, DEVICE, epoch)\n        gc.collect()\n# Configures training (and evaluation) parameters\n\ndef run():\n    if  MULTI_CORE:\n        flags = {}\n        flags['batch_size'] =  BATCH_SIZE\n        flags['num_workers'] = 8\n        flags['num_epochs'] =  NUM_EPOCHS\n        flags['seed'] = 1234\n        xmp.spawn(map_fn, args=(flags,), nprocs=8, start_method='fork')\n    else:\n        map_fn()\n    # print(flags)\nif __name__=='__main__':\n    run()","execution_count":null,"outputs":[{"output_type":"stream","text":"Loading Model and optimizer states from checkpoint....\nLoaded States !!!\nIt looks like the this states belong to epoch 1.\nso the model will train for 0 more epochs.\n\n\n######## EPOCH-0 ########\n","name":"stdout"},{"output_type":"stream","text":"  3%|▎         | 100/3472 [01:18<17:51,  3.15it/s] ","name":"stderr"},{"output_type":"stream","text":"Loss_D: 0.01443609 | Loss_G: 0.00559527 | D(x): -0.00064835 | D(G(z)): -0.00329137 / -0.00329142 | MSE: 0.00525244 | KLD: 0.00001369 | WGAN_F(G): 0.00032914 | WGAN_F(D): -0.00329142 | WGAN_R(D): 0.00064835 | WGAN_A(D): 0.01707916\n","name":"stdout"},{"output_type":"stream","text":"  6%|▌         | 200/3472 [01:49<16:47,  3.25it/s]","name":"stderr"},{"output_type":"stream","text":"Loss_D: 0.06543993 | Loss_G: 0.00205643 | D(x): -0.00423222 | D(G(z)): -0.00548476 / -0.00548473 | MSE: 0.00149599 | KLD: 0.00001197 | WGAN_F(G): 0.00054847 | WGAN_F(D): -0.00548473 | WGAN_R(D): 0.00423222 | WGAN_A(D): 0.06669244\n","name":"stdout"},{"output_type":"stream","text":"  9%|▊         | 300/3472 [02:20<16:31,  3.20it/s]","name":"stderr"},{"output_type":"stream","text":"Loss_D: 0.03462139 | Loss_G: 0.00321000 | D(x): -0.00436684 | D(G(z)): -0.00636351 / -0.00636350 | MSE: 0.00256231 | KLD: 0.00001133 | WGAN_F(G): 0.00063635 | WGAN_F(D): -0.00636350 | WGAN_R(D): 0.00436684 | WGAN_A(D): 0.03661805\n","name":"stdout"},{"output_type":"stream","text":" 12%|█▏        | 400/3472 [02:51<15:51,  3.23it/s]","name":"stderr"},{"output_type":"stream","text":"Loss_D: 0.03301423 | Loss_G: 0.00297843 | D(x): -0.00701017 | D(G(z)): -0.00893800 / -0.00893778 | MSE: 0.00207334 | KLD: 0.00001132 | WGAN_F(G): 0.00089378 | WGAN_F(D): -0.00893778 | WGAN_R(D): 0.00701017 | WGAN_A(D): 0.03494184\n","name":"stdout"},{"output_type":"stream","text":" 14%|█▍        | 500/3472 [03:21<15:26,  3.21it/s]","name":"stderr"},{"output_type":"stream","text":"Loss_D: 0.17794327 | Loss_G: 0.00361153 | D(x): -0.00704190 | D(G(z)): -0.00864531 / -0.00864533 | MSE: 0.00273524 | KLD: 0.00001176 | WGAN_F(G): 0.00086453 | WGAN_F(D): -0.00864533 | WGAN_R(D): 0.00704190 | WGAN_A(D): 0.17954671\n","name":"stdout"},{"output_type":"stream","text":" 17%|█▋        | 600/3472 [03:52<14:49,  3.23it/s]","name":"stderr"},{"output_type":"stream","text":"Loss_D: 0.10536683 | Loss_G: 0.00518783 | D(x): -0.00455053 | D(G(z)): -0.00700978 / -0.00700956 | MSE: 0.00447424 | KLD: 0.00001263 | WGAN_F(G): 0.00070096 | WGAN_F(D): -0.00700956 | WGAN_R(D): 0.00455053 | WGAN_A(D): 0.10782586\n","name":"stdout"},{"output_type":"stream","text":" 20%|██        | 700/3472 [04:23<14:16,  3.24it/s]","name":"stderr"},{"output_type":"stream","text":"Loss_D: 0.00326395 | Loss_G: 0.00309915 | D(x): -0.00871344 | D(G(z)): -0.01051914 / -0.01051904 | MSE: 0.00203537 | KLD: 0.00001188 | WGAN_F(G): 0.00105190 | WGAN_F(D): -0.01051904 | WGAN_R(D): 0.00871344 | WGAN_A(D): 0.00506955\n","name":"stdout"},{"output_type":"stream","text":" 23%|██▎       | 800/3472 [04:54<13:45,  3.24it/s]","name":"stderr"},{"output_type":"stream","text":"Loss_D: 0.02265310 | Loss_G: 0.00500074 | D(x): -0.00429392 | D(G(z)): -0.00740372 / -0.00740364 | MSE: 0.00424714 | KLD: 0.00001324 | WGAN_F(G): 0.00074036 | WGAN_F(D): -0.00740364 | WGAN_R(D): 0.00429392 | WGAN_A(D): 0.02576282\n","name":"stdout"},{"output_type":"stream","text":" 26%|██▌       | 900/3472 [05:27<13:26,  3.19it/s]","name":"stderr"},{"output_type":"stream","text":"Loss_D: 0.01740765 | Loss_G: 0.00328792 | D(x): 0.00267292 | D(G(z)): 0.00017949 / 0.00017941 | MSE: 0.00329465 | KLD: 0.00001121 | WGAN_F(G): -0.00001794 | WGAN_F(D): 0.00017941 | WGAN_R(D): -0.00267292 | WGAN_A(D): 0.01990117\n","name":"stdout"},{"output_type":"stream","text":" 29%|██▉       | 1000/3472 [05:58<12:42,  3.24it/s]","name":"stderr"},{"output_type":"stream","text":"Loss_D: 0.01829929 | Loss_G: 0.00572035 | D(x): 0.00000378 | D(G(z)): -0.00330666 / -0.00330668 | MSE: 0.00537677 | KLD: 0.00001291 | WGAN_F(G): 0.00033067 | WGAN_F(D): -0.00330668 | WGAN_R(D): -0.00000378 | WGAN_A(D): 0.02160975\n","name":"stdout"},{"output_type":"stream","text":" 32%|███▏      | 1100/3472 [06:29<12:13,  3.23it/s]","name":"stderr"},{"output_type":"stream","text":"Loss_D: 0.02885067 | Loss_G: 0.00136784 | D(x): -0.00035925 | D(G(z)): -0.00172110 / -0.00172101 | MSE: 0.00118627 | KLD: 0.00000947 | WGAN_F(G): 0.00017210 | WGAN_F(D): -0.00172101 | WGAN_R(D): 0.00035925 | WGAN_A(D): 0.03021244\n","name":"stdout"},{"output_type":"stream","text":" 35%|███▍      | 1200/3472 [07:00<11:52,  3.19it/s]","name":"stderr"},{"output_type":"stream","text":"Loss_D: 0.01186235 | Loss_G: 0.00519455 | D(x): 0.00048600 | D(G(z)): -0.00245960 / -0.00245937 | MSE: 0.00493589 | KLD: 0.00001272 | WGAN_F(G): 0.00024594 | WGAN_F(D): -0.00245937 | WGAN_R(D): -0.00048600 | WGAN_A(D): 0.01480772\n","name":"stdout"},{"output_type":"stream","text":" 37%|███▋      | 1300/3472 [07:32<11:15,  3.22it/s]","name":"stderr"},{"output_type":"stream","text":"Loss_D: 0.00703790 | Loss_G: 0.00193879 | D(x): 0.00140477 | D(G(z)): -0.00010486 / -0.00010485 | MSE: 0.00191614 | KLD: 0.00001216 | WGAN_F(G): 0.00001049 | WGAN_F(D): -0.00010485 | WGAN_R(D): -0.00140477 | WGAN_A(D): 0.00854752\n","name":"stdout"},{"output_type":"stream","text":" 40%|████      | 1400/3472 [08:02<10:45,  3.21it/s]","name":"stderr"},{"output_type":"stream","text":"Loss_D: 0.04071800 | Loss_G: 0.00430467 | D(x): 0.00517391 | D(G(z)): 0.00188279 / 0.00188272 | MSE: 0.00448030 | KLD: 0.00001264 | WGAN_F(G): -0.00018827 | WGAN_F(D): 0.00188272 | WGAN_R(D): -0.00517391 | WGAN_A(D): 0.04400920\n","name":"stdout"},{"output_type":"stream","text":" 43%|████▎     | 1500/3472 [08:34<10:14,  3.21it/s]","name":"stderr"},{"output_type":"stream","text":"Loss_D: 0.03734224 | Loss_G: 0.00267973 | D(x): 0.00640244 | D(G(z)): 0.00382764 / 0.00382766 | MSE: 0.00304860 | KLD: 0.00001389 | WGAN_F(G): -0.00038277 | WGAN_F(D): 0.00382766 | WGAN_R(D): -0.00640244 | WGAN_A(D): 0.03991703\n","name":"stdout"},{"output_type":"stream","text":" 46%|████▌     | 1600/3472 [09:04<09:44,  3.20it/s]","name":"stderr"},{"output_type":"stream","text":"Loss_D: 0.00637902 | Loss_G: 0.00140540 | D(x): 0.00798287 | D(G(z)): 0.00645625 / 0.00645616 | MSE: 0.00203689 | KLD: 0.00001413 | WGAN_F(G): -0.00064562 | WGAN_F(D): 0.00645616 | WGAN_R(D): -0.00798287 | WGAN_A(D): 0.00790573\n","name":"stdout"},{"output_type":"stream","text":" 49%|████▉     | 1700/3472 [09:36<09:16,  3.18it/s]","name":"stderr"},{"output_type":"stream","text":"Loss_D: 0.01748156 | Loss_G: 0.00333506 | D(x): 0.00845416 | D(G(z)): 0.00515166 / 0.00515174 | MSE: 0.00383690 | KLD: 0.00001333 | WGAN_F(G): -0.00051517 | WGAN_F(D): 0.00515174 | WGAN_R(D): -0.00845416 | WGAN_A(D): 0.02078398\n","name":"stdout"},{"output_type":"stream","text":" 52%|█████▏    | 1800/3472 [10:07<08:46,  3.18it/s]","name":"stderr"},{"output_type":"stream","text":"Loss_D: 0.02822577 | Loss_G: 0.00072233 | D(x): 0.00739532 | D(G(z)): 0.00603169 / 0.00603163 | MSE: 0.00131363 | KLD: 0.00001187 | WGAN_F(G): -0.00060316 | WGAN_F(D): 0.00603163 | WGAN_R(D): -0.00739532 | WGAN_A(D): 0.02958946\n","name":"stdout"},{"output_type":"stream","text":" 55%|█████▍    | 1900/3472 [10:38<08:15,  3.17it/s]","name":"stderr"},{"output_type":"stream","text":"Loss_D: 0.00624681 | Loss_G: 0.00190393 | D(x): 0.00960845 | D(G(z)): 0.00698490 / 0.00698503 | MSE: 0.00259025 | KLD: 0.00001219 | WGAN_F(G): -0.00069850 | WGAN_F(D): 0.00698503 | WGAN_R(D): -0.00960845 | WGAN_A(D): 0.00887023\n","name":"stdout"},{"output_type":"stream","text":" 58%|█████▊    | 2000/3472 [11:09<07:43,  3.17it/s]","name":"stderr"},{"output_type":"stream","text":"Loss_D: 0.01904126 | Loss_G: 0.00144547 | D(x): 0.00591287 | D(G(z)): 0.00384131 / 0.00384128 | MSE: 0.00181679 | KLD: 0.00001281 | WGAN_F(G): -0.00038413 | WGAN_F(D): 0.00384128 | WGAN_R(D): -0.00591287 | WGAN_A(D): 0.02111285\n","name":"stdout"},{"output_type":"stream","text":" 60%|██████    | 2100/3472 [11:40<07:09,  3.20it/s]","name":"stderr"},{"output_type":"stream","text":"Loss_D: 0.02405037 | Loss_G: 0.00376421 | D(x): 0.00930608 | D(G(z)): 0.00564807 / 0.00564809 | MSE: 0.00431727 | KLD: 0.00001176 | WGAN_F(G): -0.00056481 | WGAN_F(D): 0.00564809 | WGAN_R(D): -0.00930608 | WGAN_A(D): 0.02770836\n","name":"stdout"},{"output_type":"stream","text":" 63%|██████▎   | 2200/3472 [12:11<06:36,  3.21it/s]","name":"stderr"},{"output_type":"stream","text":"Loss_D: 0.01523497 | Loss_G: 0.00286786 | D(x): 0.00904536 | D(G(z)): 0.00613711 / 0.00613714 | MSE: 0.00346908 | KLD: 0.00001250 | WGAN_F(G): -0.00061371 | WGAN_F(D): 0.00613714 | WGAN_R(D): -0.00904536 | WGAN_A(D): 0.01814318\n","name":"stdout"},{"output_type":"stream","text":" 66%|██████▌   | 2300/3472 [12:42<06:07,  3.19it/s]","name":"stderr"},{"output_type":"stream","text":"Loss_D: 0.02549177 | Loss_G: 0.00247224 | D(x): 0.00618219 | D(G(z)): 0.00382088 / 0.00382089 | MSE: 0.00284206 | KLD: 0.00001227 | WGAN_F(G): -0.00038209 | WGAN_F(D): 0.00382089 | WGAN_R(D): -0.00618219 | WGAN_A(D): 0.02785307\n","name":"stdout"},{"output_type":"stream","text":" 69%|██████▉   | 2400/3472 [13:13<05:36,  3.19it/s]","name":"stderr"},{"output_type":"stream","text":"Loss_D: 0.01196991 | Loss_G: 0.00417518 | D(x): 0.00424047 | D(G(z)): 0.00093941 / 0.00093943 | MSE: 0.00425637 | KLD: 0.00001275 | WGAN_F(G): -0.00009394 | WGAN_F(D): 0.00093943 | WGAN_R(D): -0.00424047 | WGAN_A(D): 0.01527096\n","name":"stdout"},{"output_type":"stream","text":" 72%|███████▏  | 2500/3472 [13:44<05:05,  3.18it/s]","name":"stderr"},{"output_type":"stream","text":"Loss_D: 0.00680956 | Loss_G: 0.00489796 | D(x): 0.00543607 | D(G(z)): 0.00169227 / 0.00169227 | MSE: 0.00505540 | KLD: 0.00001179 | WGAN_F(G): -0.00016923 | WGAN_F(D): 0.00169227 | WGAN_R(D): -0.00543607 | WGAN_A(D): 0.01055337\n","name":"stdout"},{"output_type":"stream","text":" 75%|███████▍  | 2600/3472 [14:16<04:30,  3.22it/s]","name":"stderr"},{"output_type":"stream","text":"Loss_D: 0.00359157 | Loss_G: 0.00408825 | D(x): 0.00775151 | D(G(z)): 0.00451842 / 0.00451832 | MSE: 0.00452956 | KLD: 0.00001053 | WGAN_F(G): -0.00045183 | WGAN_F(D): 0.00451832 | WGAN_R(D): -0.00775151 | WGAN_A(D): 0.00682475\n","name":"stdout"},{"output_type":"stream","text":" 78%|███████▊  | 2700/3472 [14:47<04:00,  3.22it/s]","name":"stderr"},{"output_type":"stream","text":"Loss_D: -0.00037056 | Loss_G: 0.00323599 | D(x): 0.00588384 | D(G(z)): 0.00338734 / 0.00338740 | MSE: 0.00356417 | KLD: 0.00001056 | WGAN_F(G): -0.00033874 | WGAN_F(D): 0.00338740 | WGAN_R(D): -0.00588384 | WGAN_A(D): 0.00212589\n","name":"stdout"},{"output_type":"stream","text":" 81%|████████  | 2800/3472 [15:18<03:32,  3.16it/s]","name":"stderr"},{"output_type":"stream","text":"Loss_D: 0.00961222 | Loss_G: 0.00238491 | D(x): 0.00547094 | D(G(z)): 0.00330648 / 0.00330663 | MSE: 0.00270595 | KLD: 0.00000962 | WGAN_F(G): -0.00033066 | WGAN_F(D): 0.00330663 | WGAN_R(D): -0.00547094 | WGAN_A(D): 0.01177654\n","name":"stdout"},{"output_type":"stream","text":" 84%|████████▎ | 2900/3472 [15:50<03:02,  3.14it/s]","name":"stderr"},{"output_type":"stream","text":"Loss_D: -0.00023033 | Loss_G: 0.00505923 | D(x): 0.01161445 | D(G(z)): 0.00813605 / 0.00813581 | MSE: 0.00586256 | KLD: 0.00001026 | WGAN_F(G): -0.00081358 | WGAN_F(D): 0.00813581 | WGAN_R(D): -0.01161445 | WGAN_A(D): 0.00324831\n","name":"stdout"},{"output_type":"stream","text":" 86%|████████▋ | 3000/3472 [16:21<02:28,  3.17it/s]","name":"stderr"},{"output_type":"stream","text":"Loss_D: -0.00066302 | Loss_G: 0.00808441 | D(x): 0.01204837 | D(G(z)): 0.00746851 / 0.00746837 | MSE: 0.00882051 | KLD: 0.00001073 | WGAN_F(G): -0.00074684 | WGAN_F(D): 0.00746837 | WGAN_R(D): -0.01204837 | WGAN_A(D): 0.00391698\n","name":"stdout"},{"output_type":"stream","text":" 89%|████████▉ | 3100/3472 [16:52<01:55,  3.21it/s]","name":"stderr"},{"output_type":"stream","text":"Loss_D: 0.02601753 | Loss_G: 0.00161393 | D(x): 0.01119963 | D(G(z)): 0.00843883 / 0.00843886 | MSE: 0.00244419 | KLD: 0.00001363 | WGAN_F(G): -0.00084389 | WGAN_F(D): 0.00843886 | WGAN_R(D): -0.01119963 | WGAN_A(D): 0.02877829\n","name":"stdout"},{"output_type":"stream","text":" 92%|█████████▏| 3200/3472 [17:23<01:25,  3.19it/s]","name":"stderr"},{"output_type":"stream","text":"Loss_D: 0.02180595 | Loss_G: 0.00148547 | D(x): 0.01028687 | D(G(z)): 0.00806926 / 0.00806929 | MSE: 0.00227811 | KLD: 0.00001429 | WGAN_F(G): -0.00080693 | WGAN_F(D): 0.00806929 | WGAN_R(D): -0.01028687 | WGAN_A(D): 0.02402353\n","name":"stdout"},{"output_type":"stream","text":" 95%|█████████▌| 3300/3472 [17:54<00:53,  3.21it/s]","name":"stderr"},{"output_type":"stream","text":"Loss_D: 0.00391066 | Loss_G: 0.00196430 | D(x): 0.01150701 | D(G(z)): 0.00933607 / 0.00933610 | MSE: 0.00288512 | KLD: 0.00001279 | WGAN_F(G): -0.00093361 | WGAN_F(D): 0.00933610 | WGAN_R(D): -0.01150701 | WGAN_A(D): 0.00608157\n","name":"stdout"},{"output_type":"stream","text":" 98%|█████████▊| 3400/3472 [18:25<00:23,  3.13it/s]","name":"stderr"},{"output_type":"stream","text":"Loss_D: 0.00819048 | Loss_G: 0.00147069 | D(x): 0.01349594 | D(G(z)): 0.01083410 / 0.01083406 | MSE: 0.00254234 | KLD: 0.00001175 | WGAN_F(G): -0.00108341 | WGAN_F(D): 0.01083406 | WGAN_R(D): -0.01349594 | WGAN_A(D): 0.01085236\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 3472/3472 [18:48<00:00,  3.08it/s]\n","name":"stderr"},{"output_type":"stream","text":"Saving Model and Optimizer weights.....\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls ./checkpoint\n\n","execution_count":4,"outputs":[{"output_type":"stream","text":"__notebook_source__.ipynb\r\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}